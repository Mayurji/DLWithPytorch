{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "PyTorch is a popular open source machine learning library based on Torch library. Pytorch provides three set of libraries, i.e., torchvision, torchaudio, torchtext for Computer Vision, Audio and Text respectively.\n",
    "\n",
    "It provides two high-level features:\n",
    "\n",
    "* Tensor computation (like NumPy) with strong GPU acceleration.\n",
    "* Deep neural networks built on a type-based autograd system.\n",
    "\n",
    "**Topic Covered**\n",
    "\n",
    "- Building simple model.\n",
    "- Computing derivative.\n",
    "- Training and computing loss.\n",
    "- Using Autograd.\n",
    "- Turn tensor to a learnable parameter.\n",
    "- Training and Updating parameters using Autograd.\n",
    "\n",
    "**Note**: I am assuming the reader has basic understanding of simple machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a Model\n",
    "\n",
    "* Creating linear regression\n",
    "* Mapping of x and y.\n",
    "* Turing List into Tensor.\n",
    "* Create a function for basic line equation\n",
    "* Create a function to measure the loss using mean square error.\n",
    "\n",
    "\n",
    "Declaring vectors t_c (temperature in celsius) and t_u (unknown variable along which the temp in celsius changes).\n",
    "To be precise, t_u is the x, i.e. feature and t_c is the target/label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Python function returns the line equation, where w represents the weight vector and b represents intercept(bias term)\"\"\"\n",
    "\n",
    "def model(t_u, w, b):\n",
    "    return w*t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loss function - To find difference between estimated vs actual value. \n",
    "Loss function is called as Mean Squared Error\"\"\"\n",
    "\n",
    "def loss(t_p, t_c):\n",
    "    return torch.mean((t_p-t_c)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializing Weight and Bias\"\"\"\n",
    "\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = model(w, t_u, b)\n",
    "Loss = loss(t_p, t_c)\n",
    "print(f'Loss on Untrained Model: {Loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, the objective is to build a optimized function to estimate the acual target. Here, the optimized function is a function of weight, bias, and input. Thus, we optimize the weight and bias of the function, to accurately estimate the target.\n",
    "\n",
    "To optimize the parameters, we use the popular gradient descent algorithm, the idea is to compute the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss.\n",
    "\n",
    " The loss function represents the difference between our estimated output against the actual ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Rate Of Change In W and b**\n",
    "\n",
    "This is saying that in the neighborhood of the current values of w and b, a unit\n",
    "increase in w leads to some change in the loss. If the change is negative, then we need\n",
    "to increase w to minimize the loss, whereas if the change is positive, we need to\n",
    "decrease w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "loss_rate_of_change_w = (loss(model(t_u, w + delta, b), t_c) - loss(model(t_u, w - delta, b), t_c)) / (2.0 * delta)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "w = w - learning_rate * loss_rate_of_change_w\n",
    "\n",
    "print(f'Updated W: {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = \\\n",
    "(loss(model(t_u, w, b + delta), t_c) -\n",
    "loss(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "b = b - learning_rate * loss_rate_of_change_b\n",
    "print(f'Updated b: {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Derivative\n",
    "\n",
    "To compute derivative of the loss w.r.t parameters, we can apply chain rule(partial derivatives). Would encourage users to read through basic calculus.\n",
    "\n",
    "d_loss / d w = (d loss / d t_p) * (d t_p / d w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss(t_p, t_c):\n",
    "    dsq_diff = 2 * (t_p - t_c) / t_p.size(0)\n",
    "    \n",
    "    return dsq_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Differentiating linear equation w.r.t w parameter, linear equation is our objective function,\n",
    "   d(w*t_u + b)/ dw = t_u + 0 = t_u\"\"\"\n",
    "\n",
    "def d_model_dw(w, t_u, b):\n",
    "    return t_u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Differentiating linear equation w.r.t b parameter, d(w*t_u + b)/ db = 0 + 1.0 = 1.0\"\"\"\n",
    "\n",
    "def d_model_db(w, t_u, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"grad_fn - Updating the weight and bias using learning rate after calculating the loss.\"\"\"\n",
    "\n",
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    \n",
    "    dloss_dtp = d_loss(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * d_model_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * d_model_db(t_u, w, b)\n",
    "    \n",
    "\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Training Loop**\n",
    "\n",
    "To iterate over the training data to learn parameters(w and b) by updating it based on the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(0, n_epochs + 1):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, w, b) #Forward pass\n",
    "        \n",
    "        Loss = loss(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b) #Backward pass\n",
    "        \n",
    "        params = params - learning_rate * grad\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(Loss)))\n",
    "            print(f'params: {params}')\n",
    "            print(f'grad: {grad}\\n')\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are iterating through objective function for 100 epochs, meaning in each epoch we pass the entire dataset through objective function. We calculate the loss and update the parameters in each iteration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = training_loop(n_epochs = 100, learning_rate = 1e-2, \n",
    "\n",
    "                       params = torch.tensor([1.0, 0.0]), t_u = t_u, t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each iteration, we see the change in the parameter value of W and b w.r.t loss. But we can notice that the parameters are not updated after the 20th epoch, meaning, that the parameters are at its best minimum possible in the current experiment setup.\n",
    "\n",
    "### PyTorch - Autograd Function\n",
    "\n",
    "Pytorch provides a **.grad** characteristic to each tensor. If a tensor is created as **require_grad = True**, then that tensor turns into learnable parameter. We can check if the parameters are getting updated after executing loss.backward(). \n",
    "\n",
    "Loss.backward() tells pytorch to update the all learnable parameters to update weight based on loss. We can turn **.grad** to zero by **zero_()** because if we don't turn the **.grad to zero then grad values gets accumulated into .grad after each epoch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0],requires_grad=True)\n",
    "print(f'Learnable Parameter: {params.grad is None}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = loss(model(t_u, *params), t_c)\n",
    "Loss.backward()\n",
    "print(f'Parameter\\'s Gradient: {params.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()\n",
    "print(f'Turning Gradient to Zero: {params.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_AG(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "            \n",
    "        t_p = model(t_u, *params)\n",
    "        Loss = loss(t_p, t_c)\n",
    "        Loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'epoch {epoch}, Loss: {Loss}')\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"scaling Down the feature.\"\"\"\n",
    "t_un = 0.1 * t_u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop_AG(n_epochs = 100, learning_rate = 1e-2,\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True), t_u = t_un, t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, we can notice at the 100th epoch, the scratch code we've written earlier produces a loss of loss 29.66, while the result from the autograd gives a loss of 22.14. \n",
    "\n",
    "Increase the number of epochs for better results, because we can see that after each epoch the loss is continuously decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks For Reading. For Feedback, reach out on Github. Please don't spam."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
