{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "PyTorch is a popular open source machine learning library based on Torch library. Pytorch provides three set of libraries, i.e., torchvision, torchaudio, torchtext for Computer Vision, Audio and Text respectively.\n",
    "\n",
    "It provides two high-level features:\n",
    "\n",
    "* Tensor computation (like NumPy) with strong GPU acceleration.\n",
    "* Deep neural networks built on a type-based autograd system.\n",
    "\n",
    "**Topic Covered**\n",
    "\n",
    "- Using Pytorch Optimizer.\n",
    "- Splitting Dataset.\n",
    "- Training without no_grad.\n",
    "- Training with no_grad.\n",
    "- Creating Polynomial Model.\n",
    "- Building Neural Network Using nn.Module.\n",
    "- Building Neural Network With One Hidden Layer.\n",
    "- Finding total number of parameters in the model.\n",
    "- Building Sequential Model using OrderedDict and Named Layers.\n",
    "- Training and Predicting on validation set.\n",
    "\n",
    "**Note**: I am assuming the reader has basic understanding of simple machine learning model. Would encourage the user to read notebook V to have clear understanding of what are the different terms like model, loss function, autograd etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Optimizer are the algorithms used to update the weights. Pytorch provides wide range of optimizer from SGD to ADAM & many more. Check docstring for more details.\n",
    "* Iterate through epochs.\n",
    "* Training the model and finding the predicted value.\n",
    "* Pass the predicted value to loss function to calculate the loss.\n",
    "* Turn the existing parameter/weights to zero.\n",
    "* calculate the weights using optimizer based on loss.\n",
    "* Apply the weights update to parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Declaring vectors t_c (temperature in celsius) and t_u (unknown variable along which the temp in celsius changes).\n",
    "To be precise, t_u is the x, i.e. feature and t_c is the target/label.\n",
    "\"\"\"\n",
    "\n",
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Python function returns the line equation, where w represents the weight vector and b represents intercept(bias term)\"\"\"\n",
    "\n",
    "def model(t_u, w, b):\n",
    "    return w*t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loss function - To find difference between estimated vs actual value. \n",
    "Loss function is called as Mean Squared Error\"\"\"\n",
    "\n",
    "def loss(t_p, t_c):\n",
    "    return torch.mean((t_p-t_c)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_optim(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        t_p = model(t_u, *params)\n",
    "        Loss = loss(t_p, t_c)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {Loss}')\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"scaling Down the feature.\"\"\"\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop_optim(n_epochs=100, optimizer=optimizer,params= params,  t_u = t_un,t_c= t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataset into Train and Validation\n",
    "\n",
    "* We use indexing to split the dataset. Using random selection of indices, we select the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "print(f'Indices of Training Samples: {train_indices}')\n",
    "print(f'Indices of validation Samples: {val_indices}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scaling Down the training and validation samples by 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u, train_t_c = t_u[train_indices], t_c[train_indices]\n",
    "val_t_u, val_t_c = t_u[val_indices], t_c[val_indices] \n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, we write the training_loop_grad method to train the model and simultaneously validate the model on validation dataset.\n",
    "\n",
    "Note, While performing an inference on the validation set, we should disable the gradients calculation. Also note the loss at the end of 100th epoch.\n",
    "\n",
    "[PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.no_grad.html)\n",
    ">Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_grad(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss(train_t_p, train_t_c)\n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss = loss(val_t_p, val_t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Validation loss {val_loss.item():.4f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop_grad(n_epochs = 100, optimizer = optimizer,\n",
    "params = params, train_t_u = train_t_un, val_t_u = val_t_un,\n",
    "train_t_c = train_t_c, val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, we write the training_loop_no_grad method to train the model and simultaneously validate the model on validation dataset. Here, by using torch.no_grad(), we disable the gradients.\n",
    "\n",
    "Check out `torch.set_grad_enabled`  for enabling and disabiling gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_no_grad(n_epochs, optimizer, params, train_t_u, val_t_u,train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Validation loss {val_loss.item():.4f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop_no_grad(n_epochs = 100, optimizer = optimizer,\n",
    "params = params, train_t_u = train_t_un, val_t_u = val_t_un,\n",
    "train_t_c = train_t_c, val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Equation - w2 * t_u ** 2 + w1 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Three parameters - w2, w1, b. Input t_u and its square(t_u**2).\n",
    "\n",
    "For instance, equation can be thought as 2X^2 + 4X + 5, the coefficient (2, 4, 5) are (w2, w1, b).\n",
    "\n",
    "We are using in-built methods for loss calculation and for updating the parameters.\n",
    "\"\"\"\n",
    "\n",
    "params = torch.randn(3,) \n",
    "params.requires_grad=True\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam([params], lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyModel(t_u, w1, w2, b):\n",
    "    return w2 * t_u ** 2 + w1 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, params, train_t_u, val_t_u,train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w1, w2, b = params\n",
    "        train_t_p = polyModel(train_t_u, w1, w2, b)\n",
    "        train_loss = criterion(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = polyModel(val_t_u, *params)\n",
    "            val_loss = criterion(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10==0:\n",
    "            print(f'Epoch: {epoch}, Loss: {train_loss}, Parameters: {params} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(n_epochs = 100, optimizer = optimizer, params=params,\n",
    "train_t_u = train_t_un, val_t_u = val_t_un,\n",
    "train_t_c = train_t_c, val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** With increasing complexity of the model(polynomial), the performance of the model on that training data is improved. In polynomial model at epoch 10, we have a training loss of 4.48, while for the linear model at epoch 10, we have a training loss of 20.08."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Nets Using nn.Module\n",
    "\n",
    "**nn.Module** is the base class of all neural network modules. Here, the neural network modules refers to the layers like linear layers, convolution layers, LSTM layers, etc, which are required to build the neural network architecture.\n",
    "\n",
    "For our previous linear model, we used one feature and a target variable. Here, we'll build the same model using nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dummy Linear Equation model with one sample with one feature. \n",
    "We can notice that by declaring the Linear Layer, \n",
    "we automatically create weight tensor and the bias term is optional\"\"\"\n",
    "\n",
    "x = torch.ones(1)\n",
    "l_model = nn.Linear(1,1, bias=True)\n",
    "l_model(x)\n",
    "print(f'Weight of the Linear Layer: {l_model.weight.item()} \\n')\n",
    "print(f'Bias of the Linear Layer: {l_model.bias.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model which takes batch of samples with one feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a set of sample with one value.\n",
    "\"\"\"\n",
    "x = torch.ones(10, 1)\n",
    "print(f'Passing set of samples through Linear Layer: \\n{l_model(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Network with One Hidden Layer\n",
    "\n",
    "* Pytorch has Sequence Module similar to Keras, We can add layers in sequence, in the order of operation.\n",
    "* Finding Total number of parameters in the model.\n",
    "* Assigning names to layers, it is helpful when you want to retrain particular layer.\n",
    "* Training the network with optimizer and criterion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t_u.unsqueeze(1)\n",
    "y = t_c.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Model**\n",
    "\n",
    "* First Linear Layer is the hidden layer with 10 neurons.\n",
    "* Tanh is the activation function.\n",
    "* Second Linear layer is the output layer with 1 neuron (Label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(1, 10),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(10, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total Number Of Parameters: {sum(p.numel() for p in seq_model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(f'Parameter Detail: {name, param.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Model Using OrderedDict With Named Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = nn.Sequential(OrderedDict([\n",
    "    ('hidden_linear', nn.Linear(1, 9)),\n",
    "    ('hidden_activation', nn.Tanh()),\n",
    "    ('output_linear', nn.Linear(9, 1))\n",
    "    ]))\n",
    "print(seq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Named Layers\"\"\"\n",
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Random Initialization Of Weight and Bias Term\"\"\"\n",
    "\n",
    "print(f'Weight Initialized: {seq_model.output_linear.weight}')\n",
    "print(f'Bias Initialized: {seq_model.output_linear.bias.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,\n",
    "t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        with torch.no_grad():\n",
    "            t_p_val = model(t_u_val)\n",
    "            loss_val = loss_fn(t_p_val, t_c_val)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
    "            f\" Validation loss {loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Trained Sequential Model to predict on the validation samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(n_epochs = 100, optimizer = optimizer,model = seq_model, loss_fn = nn.MSELoss(),\n",
    "t_u_train = train_t_un.unsqueeze(1), t_u_val = val_t_un.unsqueeze(1),\n",
    "t_c_train = train_t_c.unsqueeze(1), t_c_val = val_t_c.unsqueeze(1))\n",
    "\n",
    "print('\\nValidation Set\\'s Predicted Output: ', seq_model(val_t_un.unsqueeze(1)),\"\\n\")\n",
    "print('Validation Set\\'s Ground Truth:', val_t_c.unsqueeze(1), '\\n')\n",
    "print('Hidden Tensor Weights:', seq_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks For Reading. For Feedback, reach out on Github. Please don't spam."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
