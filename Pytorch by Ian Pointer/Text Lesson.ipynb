{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embedding Matrix\n",
    "\n",
    "Sentence: The cat sat on the mat.\n",
    "\n",
    "```python\n",
    "embed = nn.Embedding(vocab_size, dimension_size) # Declaration of Embedding Matrix\n",
    "cat_mat_embed = nn.Embedding(5, 2)\n",
    "cat_tensor = Tensor([1])\n",
    "cat_mat_embed.forward(cat_tensor)\n",
    "\n",
    "> tensor([[ 1.7793, -0.3127]], grad_fn=<EmbeddingBackward>)\n",
    "```\n",
    "\n",
    "Note: Pandas uses C based Csv Parser and may end up throwing error like utf-8 can't decode error. We can switch to python based CSV parser by setting\n",
    "engine parameter in pandas .read_csv as 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = data.LabelField()\n",
    "TEXT = data.Field(tokenize='spacy', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/mayur/Desktop/Kaggle Notebooks/NLP-getting-started/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'text': ('text', TEXT),'target': ('label', LABEL)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.TabularDataset(\n",
    "        path=PATH+\"train.json\",\n",
    "        format=\"json\",\n",
    "        fields=fields,\n",
    "        skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = data.TabularDataset(\n",
    "        path=PATH+\"valid.json\",\n",
    "        format=\"json\",\n",
    "        fields=fields,\n",
    "        skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = data.Field()\n",
    "test_fields = {'text': ('text', TEXT),'id': ('id', ID)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data.TabularDataset(\n",
    "        path=PATH+\"test.json\",\n",
    "        format=\"json\",\n",
    "        fields=test_fields,\n",
    "        skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['ebola',\n",
       "  'case',\n",
       "  'possible',\n",
       "  'home',\n",
       "  'quarantined',\n",
       "  'alabama',\n",
       "  'officials'],\n",
       " 'id': ['7875']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(test.examples[2354])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train,max_size = 15000)\n",
    "\n",
    "LABEL.build_vocab(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((train, valid, None),batch_sizes=(64, 128, 0),\n",
    "                                                            sort_key=lambda x: len(x.text), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(test)\n",
    "ID.build_vocab(test)\n",
    "test_iter = data.Iterator(dataset=test, device=device, batch_size=128, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_NET(\n",
       "  (embedding): Embedding(15002, 300)\n",
       "  (encoder): LSTM(300, 100)\n",
       "  (predictor): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM_NET(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1)\n",
    "        self.predictor = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        output, (hidden,_) = self.encoder(self.embedding(seq))\n",
    "        preds = self.predictor(hidden.squeeze(0))\n",
    "        return preds\n",
    "    \n",
    "model = LSTM_NET(100, 300, 15002)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.002)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, optimizer, criterion, train_iter, val_iter):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss=0.0\n",
    "        validation_loss=0.0\n",
    "        model.train()\n",
    "        for batch_id, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(batch.text)\n",
    "            batch.label = batch.label.to(torch.float32)\n",
    "            loss = criterion(predict.squeeze(1), batch.label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * batch.text.size(0)\n",
    "        training_loss /=len(train_iter)\n",
    "        \n",
    "        model.eval()\n",
    "        for batch_id, batch in enumerate(val_iter):\n",
    "            predict = model(batch.text)\n",
    "            batch.label = batch.label.to(torch.float32)\n",
    "            loss = criterion(predict.squeeze(1), batch.label)\n",
    "            validation_loss += loss.data.item() * batch.text.size(0)\n",
    "            \n",
    "        validation_loss /= len(val_iter)\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}'.format(epoch, training_loss, validation_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=100, model=model, optimizer=optimizer, criterion=criterion, train_iter=train_iter, val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'tweet_disaster.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "def classify_tweet(tweet):\n",
    "    categories = {0: \"Negative\", 1:\"Positive\"}\n",
    "    processed = TEXT.process([TEXT.preprocess(tweet)])\n",
    "    return categories[model(processed).argmax().item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    ">**Random Insertion** -  random insertion technique looks at a sentence and then randomly inserts synonyms of existing nonstop-words into the sentence n times. Assuming you have a way of getting a synonym of a word and a way of eliminating stop-words (common words such as and, it, the, etc.), shown, but not implemented, in this function via get_synonyms() and get_stopwords(), an implementation of this would be as follows:\n",
    "\n",
    "```python\n",
    "def random_insertion(sentence,n):\n",
    "    words = remove_stopwords(sentence)\n",
    "    for _ in range(n):\n",
    "        new_synonym = get_synonyms(random.choice(words))\n",
    "        sentence.insert(randrange(len(sentence)+1), new_synonym)\n",
    "    return sentence\n",
    "         \n",
    "```\n",
    "\n",
    "    An example of this in practice where it replaces cat could look like this:\n",
    "\n",
    "    The cat sat on the mat\n",
    "    The cat mat sat on feline the mat\n",
    "\n",
    ">**Random Deletion** - As the name suggests, random deletion deletes words from a sentence. Given a probability parameter p, it will go through the sentence and decide whether to delete a word or not based on that random probability:\n",
    "\n",
    "```python\n",
    " def random_deletion(words, p=0.5):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words))\n",
    "    if len(remaining) == 0:\n",
    "        return [random.choice(words)]\n",
    "    else\n",
    "        return remaining\n",
    "```\n",
    "    The implementation deals with the edge cases—if there’s only one word, the technique returns it; and if we end up deleting all the words in the sentence, the technique samples a random word from the original set.\n",
    "\n",
    ">**Random Swap** - The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here’s an implementation:\n",
    "\n",
    "```python\n",
    " def random_swap(sentence, n=5):\n",
    "    length = range(len(sentence))\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(length, 2)\n",
    "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1]\n",
    "    return sentence\n",
    "\n",
    "```\n",
    "    We sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n.\n",
    "\n",
    "    The techniques in the EDA paper average about a 3% improvement in accuracy when used with small amounts of labeled examples (roughly 500). If you have more than 5,000 examples in your dataset, the paper suggests that this improvement may fall to 0.8% or lower, due to the model obtaining better generalization from the larger amounts of data available over the improvements that EDA can provide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
