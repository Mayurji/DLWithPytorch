{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms.functional as transFunc\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Transforms\n",
    "transform = transforms.Compose([transforms.Resize(16), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "#Resize to 64 is updated, our images are 32 x 32!\n",
    "#Start Small and Get Bigger, means keeps the size of image 16, then 32 and so on and check the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='/home/mayur/Desktop/Pytorch/data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='/home/mayur/Desktop/Pytorch/data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in image size after resize transform.\n",
    "\n",
    "```python\n",
    "img, label = cifar_2[0]\n",
    "print(img.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Mapping - classification of two class \n",
    "\n",
    "#CIFAR has 10 classes, we are restricting it into 2 classes\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar_2 = [(img, label_map[label1]) for img, label1 in trainset if label1 in [0,2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in testset if label in [0, 2]]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(cifar_2, batch_size=64,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing exisiting Parameters, By keeping parameters as false the resnet's parameter won't under go backprop !\n",
    "for name, param in transfer_model.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP\n",
    "\n",
    "You might not want to freeze the BatchNorm layers in a model, as they will be trained to approximate the mean and standard deviation of the dataset that the model was originally trained on, not the dataset that you want to fine-tune on. Some of the signal from your data may end up being lost as BatchNorm corrects your input. You can look at the model structure and freeze only layers that arenâ€™t BatchNorm like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = models.resnet34(pretrained=True)\n",
    "for name, param in transfer_model.named_parameters():\n",
    "    if ('bn' not in name):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a New Layer to pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 512])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "for name, param in transfer_model.named_parameters():\n",
    "    if ('fc' in name):\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fc = nn.Sequential(nn.Linear(transfer_model.fc.in_features, 500), nn.ReLU(), nn.Dropout(), nn.Linear(500, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Better Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is quite simple: over the course of an epoch, start out with a small learning rate and increase to a higher learning rate over each mini-batch, resulting in a high rate at the end of the epoch. Calculate the loss for each rate and then, looking at a plot, pick the learning rate that gives the greatest decline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finding learning rate implemented in fast.ai\n",
    "\n",
    "import math\n",
    "def find_lr(model, loss_fn, optimizer, init_value=1e-8, final_value=10.0):\n",
    "    number_in_epoch = len(trainloader) - 1\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for data in trainloader:\n",
    "        batch_num += 1\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # Crash out if loss explodes\n",
    "\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            return log_lrs[10:-5], losses[10:-5]\n",
    "\n",
    "        # Record the best loss\n",
    "\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Store the values\n",
    "\n",
    "        losses.append(loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "\n",
    "        # Do the backward pass and optimize\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the lr for the next step and store\n",
    "\n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "    return log_lrs[10:-5], losses[10:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe08e0865d0>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)\n",
    "transfer_model.to(device)\n",
    "logs,losses = find_lr(transfer_model, torch.nn.CrossEntropyLoss(), optimizer)\n",
    "plt.plot(logs,losses)\n",
    "#near 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, trainloader, testloader, n_epochs, device):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            img, label = batch\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            output = model(img)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item()\n",
    "        training_loss /= len(trainset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        for batch in testloader:\n",
    "            img, label = batch\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            output = model(img)\n",
    "            loss = loss_fn(output, label)\n",
    "            valid_loss += loss.data.item()\n",
    "            correct = torch.eq(torch.max(F.softmax(output),dim=1)[1], label).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "            \n",
    "        valid_loss/=len(testset)\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Training Loss: {training_loss:.3f}, Validation Loss: {valid_loss:.3f}, Accuracy: {num_correct/num_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8635\n",
      "Epoch: 2, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.875\n",
      "Epoch: 3, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.857\n",
      "Epoch: 4, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.864\n",
      "Epoch: 5, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.882\n",
      "Epoch: 6, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8715\n",
      "Epoch: 7, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.886\n",
      "Epoch: 8, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8885\n",
      "Epoch: 9, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8875\n",
      "Epoch: 10, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8745\n",
      "Epoch: 11, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8825\n",
      "Epoch: 12, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.89\n",
      "Epoch: 13, Training Loss: 0.001, Validation Loss: 0.004, Accuracy: 0.7535\n",
      "Epoch: 14, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.883\n",
      "Epoch: 15, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8925\n",
      "Epoch: 16, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8985\n",
      "Epoch: 17, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8915\n",
      "Epoch: 18, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8985\n",
      "Epoch: 19, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8865\n",
      "Epoch: 20, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "### Pretrained Model\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr=0.01)\n",
    "transfer_model.to(device)\n",
    "train(transfer_model, optimizer, torch.nn.CrossEntropyLoss(), trainloader, testloader, n_epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune some of the layers of, say, the ResNet model weâ€™re using. Perhaps adding some training to the layers just preceding our classifier will make our model just a little more accurate. But as those preceding layers have already been trained on the ImageNet dataset, maybe they need only a little bit of training as compared to our newer layers. We are training existing layers of resnet with different learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([\n",
    "{ 'params': transfer_model.layer4.parameters(), 'lr': 1e-2 /3},\n",
    "{ 'params': transfer_model.layer3.parameters(), 'lr': 1e-2 /9},\n",
    "], lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze_layers = [transfer_model.layer3, transfer_model.layer4]\n",
    "for layer in unfreeze_layers:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.001, Validation Loss: 0.003, Accuracy: 0.6995\n",
      "Epoch: 2, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.886\n",
      "Epoch: 3, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8965\n",
      "Epoch: 4, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8985\n",
      "Epoch: 5, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.911\n",
      "Epoch: 6, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.902\n",
      "Epoch: 7, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8945\n",
      "Epoch: 8, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.903\n",
      "Epoch: 9, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.903\n",
      "Epoch: 10, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.904\n",
      "Epoch: 11, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.903\n",
      "Epoch: 12, Training Loss: 0.001, Validation Loss: 0.077, Accuracy: 0.772\n",
      "Epoch: 13, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8545\n",
      "Epoch: 14, Training Loss: 0.001, Validation Loss: 0.003, Accuracy: 0.617\n",
      "Epoch: 15, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.889\n",
      "Epoch: 16, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8965\n",
      "Epoch: 17, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.883\n",
      "Epoch: 18, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8895\n",
      "Epoch: 19, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8865\n",
      "Epoch: 20, Training Loss: 0.001, Validation Loss: 0.001, Accuracy: 0.8865\n"
     ]
    }
   ],
   "source": [
    "### Pretrained Model\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr=0.01)\n",
    "transfer_model.to(device)\n",
    "train(transfer_model, optimizer, torch.nn.CrossEntropyLoss(), trainloader, testloader, n_epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    ">**Color Jitter** - randomly changes the brightness, contrast, saturation, and hue of an image. For brightness, contrast, and saturation, you can supply either a float or a tuple of floats, all nonnegative in the range 0 to 1, and the randomness will either be between 0 and the supplied float or it will use the tuple to generate randomness between the supplied pair of floats. For hue, a float or float tuple between â€“0.5 and 0.5 is required, and it will generate random hue adjustments between [-hue,hue] or [min, max]\n",
    "```python\n",
    "torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\n",
    "```\n",
    ">**Flip** - If you want to flip your image, these two transforms randomly reflect an image on either the horizontal or vertical axis:\n",
    "```python\n",
    "torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "torchvision.transforms.RandomVerticalFlip(p=0.5)\n",
    "````\n",
    ">**RandomGrayscale** is a similar type of transformation, except that it randomly turns the image grayscale, depending on the parameter p (the default is 10%):\n",
    "```python\n",
    " torchvision.transforms.RandomGrayscale(p=0.1)\n",
    "``` \n",
    ">**RandomCrop** and **RandomResizeCrop**, as you might expect, perform random crops on the image of size, which can either be an int for height and width, or a tuple containing different heights and widths. \n",
    ">RandomResizeCrop is using Bilinear interpolation, but you can also select nearest neighbor or bicubic interpolation by changing the interpolation parameter. See the PIL filters page for further details. \n",
    ">By default, RandomResize is using constant padding, which fills out the otherwise empty pixels beyond the image with the value given in fill. However, I recommend that you use the reflect padding instead, as empirically it seems to work a little better than just throwing in empty constant space. \n",
    "```python\n",
    "torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')\n",
    "\n",
    "torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)\n",
    "```\n",
    ">**RandomRotation**, If youâ€™d like to randomly rotate an image, RandomRotation will vary between [-degrees, degrees] if degrees is a single float or int, or (min,max) if it is a tuple:\n",
    ">If expand is set to True, this function will expand the output image so that it can include the entire rotation; by default, itâ€™s set to crop to within the input dimensions. You can specify a PIL resampling filter, and optionally provide an (x,y) tuple for the center of rotation; otherwise the transform will rotate about the center of the image.\n",
    "```python\n",
    "torchvision.transforms.RandomRotation(degrees, resample=False,expand=False, center=None)\n",
    "```\n",
    ">**Pad** is a general-purpose padding transform that adds padding (extra height and width) onto the borders of an image. A single value in padding will apply padding on that length in all directions. A two-tuple padding will produce padding in the length of (left/right, top/bottom), and a four-tuple will produce padding for (left, top, right, bottom). By default, padding is set to constant mode, which copies the value of fill into the padding slots. The other choices are edge, which pads the last values of the edge of the image into the padding length; reflect, which reflects the values of the image (except the edge) into the border; and symmetric, which is reflection but includes the last value of the image at the edge. \n",
    "```python\n",
    "torchvision.transforms.Pad(padding, fill=0, padding_mode=constant)\n",
    "```\n",
    ">**RandomAffine** allows you to specify random affine translations of the image (scaling, rotations, translations, and/or shearing, or any combination).\n",
    "The degrees parameter is either a single float or int or a tuple. In single form, it produces random rotations between (â€“degrees, degrees). With a tuple, it will produce random rotations between (min, max). degrees has to be explicitly set to prevent rotations from occurringâ€”thereâ€™s no default setting. translate is a tuple of two multipliers (horizontal_multipler, vertical_multiplier). At transform time, a horizontal shift, dx, is sampled in the range â€“image_width Ã— horizontal_multiplier < dx < img_width Ã— horizontal_width, and a vertical shift is sampled in the same way with respect to the image height and the vertical multiplier. Scaling is handled by another tuple, (min, max), and a uniform scaling factor is randomly sampled from those. Shearing can be either a single float/int or a tuple, and randomly samples in the same manner as the degrees parameter. Finally, resample allows you to optionally provide a PIL resampling filter, and fillcolor is an optional int specifying a fill color for areas inside the final image that lie outside the final transform.\n",
    "```python\n",
    "torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)\n",
    "```\n",
    "    Widely Done Transformations -  Random flips, color jittering, rotation, and crops to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transforms\n",
    "\n",
    ">**Color Spaces and Lambda Transforms**, This may seem a little odd to even bring up, but so far all our image work has been in the fairly standard 24-bit RGB color space, where every pixel has an 8-bit red, green, and blue value to indicate the color of that pixel. However, other color spaces are available!\n",
    "\n",
    ">A popular alternative is HSV, which has three 8-bit values for hue, saturation, and value. Some people feel this system more accurately models human vision than the traditional RGB color space. But why does this matter? A mountain in RGB is a mountain in HSV, right?\n",
    "\n",
    ">Well, thereâ€™s some evidence from recent deep learning work in colorization that other color spaces can produce slightly higher accuracy than RGB. A mountain may be a mountain, but the tensor that gets formed in each spaceâ€™s representation will be different, and one space may capture something about your data better than another.\n",
    "\n",
    ">When combined with ensembles, you could easily create a series of models that combines the results of training on RGB, HSV, YUV, and LAB color spaces to wring out a few more percentage points of accuracy from your prediction pipeline.\n",
    "\n",
    ">One slight problem is that PyTorch doesnâ€™t offer a transform that can do this. But it does provide a couple of tools that we can use to randomly change an image from standard RGB into HSV (or another color space). First, if we look in the PIL documentation, we see that we can use Image.convert() to translate a PIL image from one color space to another. We could write a custom transform class to carry out this conversion, but PyTorch adds a transforms.Lambda class so that we can easily wrap any function and make it available to the transform pipeline. \n",
    "\n",
    "```python\n",
    "def _random_colour_space(x):\n",
    "    output = x.convert('HSV')\n",
    "    return output\n",
    "```\n",
    "\n",
    ">This is then wrapped in a transforms.Lambda class and can be used in any standard transformation pipeline like weâ€™ve seen before:\n",
    "\n",
    "```python\n",
    "colour_transform = transforms.Lambda(lambda x: _random_colour_space(x))\n",
    "```\n",
    ">Thatâ€™s fine if we want to convert every image into HSV, but really we donâ€™t want that. Weâ€™d like it to randomly change images in each batch, so itâ€™s probable that the image will be presented in different color spaces in different epochs. We could update our original function to generate a random number and use that to generate a random probability of changing the image, but instead weâ€™re even lazier and use RandomApply:\n",
    "\n",
    "```python\n",
    "random_colour_transform = torchvision.transforms.RandomApply([colour_transform])\n",
    "```\n",
    "\n",
    ">By default, RandomApply fills in a parameter p with a value of 0.5, so thereâ€™s a 50/50 chance of the transform being applied. Experiment with adding more color spaces and the probability of applying the transformation to see what effect it has on our cat and fish problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transform Classes\n",
    "\n",
    ">Sometimes a simple lambda isnâ€™t enough; maybe we have some initialization or state that we want to keep track of, for example. In these cases, we can create a custom transform that operates on either PIL image data or a tensor. Such a class has to implement two methods: \\__call__, which the transform pipeline will invoke during the transformation process; and \\__repr__, which should return a string representation of the transform, along with any state that may be useful for diagnostic purposes.\n",
    "\n",
    ">In the following code, we implement a transform class that adds random Gaussian noise to a tensor. When the class is initialized, we pass in the mean and standard distribution of the noise we require, and during the \\__call__ method, we sample from this distribution and add it to the incoming tensor:\n",
    "\n",
    "```python\n",
    "class Noise():\n",
    "    \"\"\"Adds gaussian noise to a tensor.\n",
    "\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>>     Noise(0.1, 0.05)),\n",
    "        >>> ])\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, stddev):\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.zeros_like(tensor).normal_(self.mean, self.stddev)\n",
    "        return tensor.add_(noise)\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr = f\"{self.__class__.__name__  }(mean={self.mean},\n",
    "               stddev={self.stddev})\"\n",
    "        return repr\n",
    "````\n",
    ">If we add this to a pipeline, we can see the results of the \\__repr__ method being called:\n",
    "\n",
    "```python\n",
    "transforms.Compose([Noise(0.1, 0.05)])\n",
    ">> Compose(\n",
    "    Noise(mean=0.1,sttdev=0.05)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Small and Get Bigger!**\n",
    "\n",
    "Hereâ€™s a tip that seems odd, but obtains real results: start small and get bigger. What I mean is if youâ€™re training on 256 Ã— 256 images, create a few more datasets in which the images have been scaled to 64 Ã— 64 and 128 Ã— 128. Create your model with the 64 Ã— 64 dataset, fine-tune as normal, and then train the exact same model with the 128 Ã— 128 dataset. Not from scratch, but using the parameters that have already been trained. Once it looks like youâ€™ve squeezed the most out of the 128 Ã— 128 data, move on to your target 256 Ã— 256 data. Youâ€™ll probably find a percentage point or two improvement in accuracy.\n",
    "\n",
    "While we donâ€™t know exactly why this works, the working theory is that by training at the lower resolutions, the model learns about the overall structure of the image and can refine that knowledge as the incoming images expand. But thatâ€™s just a theory. However, that doesnâ€™t stop it from being a good little trick to have up your sleeve when you need to squeeze every last bit of performance from a model.\n",
    "\n",
    "If you donâ€™t want to have multiple copies of a dataset hanging around in storage, you can use torchvision transforms to do this on the fly using the Resize function:\n",
    "```python\n",
    "resize = transforms.Compose([ transforms.Resize(64),\n",
    " â€¦_other augmentation transforms_â€¦\n",
    " transforms.ToTensor(),\n",
    " transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "```\n",
    "The penalty you pay here is that you end up spending more time in training, as PyTorch has to apply the resize every time. If you resized all the images beforehand, youâ€™d likely get a quicker training run, at the expense of filling up your hard drive. But isnâ€™t that trade-off always the way?\n",
    "\n",
    "The concept of starting small and then getting bigger also applies to architectures. Using a ResNet architecture like ResNet-18 or ResNet-34 to test out approaches to transforms and get a feel for how training is working provides a much tighter feedback loop than if you start out using a ResNet-101 or ResNet-152 model. Start small, build upward, and you can potentially reuse the smaller model runs at prediction time by adding them to an ensemble model.\n",
    "\n",
    "Ensembles\n",
    "Whatâ€™s better than one model making predictions? Well, how about a bunch of them? Ensembling is a technique that is fairly common in more traditional machine learning methods, and it works rather well in deep learning too. The idea is to obtain a prediction from a series of models, and combine those predictions to produce a final answer. Because different models will have different strengths in different areas, hopefully a combination of all their predictions will produce a more accurate result than one model alone.\n",
    "\n",
    "There are plenty of approaches to ensembles, and we wonâ€™t go into all of them here. Instead, hereâ€™s a simple way of getting started with ensembles, one that has eeked out another 1% of accuracy in my experience; simply average the predictions:\n",
    "\n",
    "**Assuming you have a list of models in models, and input is your input tensor**\n",
    "```python\n",
    "predictions = [m[i].fit(input) for i in models]\n",
    "avg_prediction = torch.stack(b).mean(0).argmax()\n",
    "```\n",
    "\n",
    "The stack method concatenates the array of tensors together, so if we were working on the cat/fish problem and had four models in our ensemble, weâ€™d end up with a 4 Ã— 2 tensor constructed from the four 1 Ã— 2 tensors. And mean does what youâ€™d expect, taking the average, although we have to pass in a dimension of 0 to ensure that it takes an average across the first dimension instead of simply adding up all the tensor elements and producing a scalar output. Finally, argmax picks out the tensor index with the highest element, as youâ€™ve seen before.\n",
    "\n",
    "Itâ€™s easy to imagine more complex approaches. Perhaps weights could be added to each individual modelâ€™s prediction, and those weights adjusted if a model gets an answer right or wrong. What models should you use? Iâ€™ve found that a combination of ResNets (e.g., 34, 50, 101) work quite well, and thereâ€™s nothing to stop you from saving your model regularly and using different snapshots of the model across time in your ensemble!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
