{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this Notebook, I will start with the same flow as mentioned in the official textbook.\n",
    "\n",
    "* How to train a pretrained model in Pytorch\n",
    "* How to work with pytorch like tensor, numpy to tensor, named tensor, storage in pytorch etc.\n",
    "* How to implement One-hot encoding in Pytorch.\n",
    "* How to handle Time series data in Pytorch\n",
    "* How to handle Images in Pytorch, changing dimension as required in pytorch module.\n",
    "* How to perform Word to Index conversion\n",
    "* How to split data between train and validation\n",
    "* How to build model in Pytorch\n",
    "* How to find derivative in pytorch\n",
    "* How to move from training mode to evaluation mode\n",
    "* How to train a network with help of Optimizer and Criterion (Loss Function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to predict a single image on a Pre-trained Model - Resnet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "Pytorch provides three set of libraries i.e. **torchvision, torchaudio, torchtext for Computer Vision, Audio and Text respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Model\n",
    "\n",
    "Using Models module from Torchvision, we can load all the pretrained models.\n",
    "I am loading a resnet34 model with **pretrained=True**, which means i will be using weights of the model on which it was trained on. \n",
    "Resnet34 was trained on Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Transforms\n",
    "\n",
    "Transforms is cool feature in torchvision, because we can apply a list of **transforms/augmentation** on an image by just simply adding it as parameter in transforms module. We can also customize other transforms if required, if its not included in **torchvision.transforms**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess= transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean = [0.5, 0.5, 0.5],\n",
    "        std = [0.2, 0.2, 0.2])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your Single File\n",
    "\n",
    "List of things happening in the below cell\n",
    "\n",
    "* I am using Python Image Library (PIL) for loading an single image.\n",
    "* Applying the transformation declared above.\n",
    "* Checking out the shape of the image.\n",
    "* Note the shape of the image, it should apply the transforms.centercrop() and resize the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('/home/mayur/Desktop/opencv_tut/Images/traffic.jpeg')\n",
    "img_p = preprocess(img)\n",
    "print(img_p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model requires batch size as first dimension, so we reshape the image dimension.\n",
    "\n",
    "Its quite a standard practice to keep the dimension of the batch size as first dimension.\n",
    "In pytorch, we use **unsqueeze** to add an dimension to existing matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t = torch.unsqueeze(img_p, 0)\n",
    "batch_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch, it is easy to switch between training phase and testing phase. \n",
    "\n",
    "**Model.eval(),** disables the batchnorm and dropout layers of a models. Similarly we have **model.train()** phase for training.\n",
    "\n",
    "**Since we are predicting using pretrained model, we use model under eval mode. \n",
    "And in eval mode, the batchNorm and Dropout layers of the model will be disabled.\n",
    "we are initializing the resnet34 model and predicting on one image under batch_t variable.\n",
    "The out variable contains our predicted output over 1000 classes. Since resnet was trained on Imagenet, which has 1000 classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "resnet.eval()\n",
    "out = resnet(batch_t)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Images Classes from txt file\n",
    "\n",
    "Below, we are loading the class names of the classes in Imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_class.txt') as f:\n",
    "    classes = [line.strip().split(\",\")[1].strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Index of the max probability class\n",
    "\n",
    "The variable **out** is a vector with 1000 elements with set of values providing weights to each class. Higher weight of the class results as predicted class of the image. Using max and dimension=1, we are fetching the index of the vector, where the weight is maximum among 1000 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = torch.max(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([920])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence of Prediction\n",
    "**Softmax function is used in Multiclass classification, it squeezes the value/weight as mentioned above between 0 and 1.** \n",
    "So the all 1000 weights are squeezed between 0 to 1 and **all summing up to 1.** We further convert the class index into label of the class and present it as confidence percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('traffic_light', 99.99995422363281)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "classes[index[0]], percentage[index[0]].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 predictions\n",
    "\n",
    "Similar to above code, only showing top five predictions of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('traffic_light', 99.99995422363281),\n",
       " ('street_sign', 2.8018390366923995e-05),\n",
       " ('pole', 9.717282409837935e-06),\n",
       " ('loudspeaker', 2.9554805678344565e-06),\n",
       " ('binoculars', 1.4750306718269712e-06)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, indices = torch.sort(out, descending=True)\n",
    "[(classes[idx], percentage[idx].item()) for idx in indices[0][:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Declare the matrix with 5x5 dimension with 3 channel.\n",
    "* Creating a vector of size 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "print(img_t.shape)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating Matrix with batch dimension in comparision with Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]\n",
    "batch_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finding mean of the matrix. Three channel are added and the average is found. Combining three channel to one channel i.e. gray scale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5]) torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "print(img_gray_naive.shape, batch_gray_naive.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding dimension to a vector to convert it into matrix, since most of the calculation in deep learning involves weight matrix multiplication.\n",
    "* Multiplying weights with one matrix.\n",
    "* Multiplying weights with batch of matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2126]],\n",
       "\n",
       "        [[0.7152]],\n",
       "\n",
       "        [[0.0722]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights.shape)\n",
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "print(unsqueezed_weights.shape)\n",
    "unsqueezed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights = (img_t * unsqueezed_weights)\n",
    "img_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "batch_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Converting three channel matrix to one channel matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3185,  0.4227,  0.5607,  0.3115, -1.0247],\n",
       "        [-0.0632, -0.5625,  1.3939, -1.1633, -0.3991],\n",
       "        [-1.1099, -0.1263, -0.4878,  0.1427, -0.5896],\n",
       "        [-0.7000,  0.7684,  1.0612, -0.0851, -0.3613],\n",
       "        [ 0.3097, -1.1336,  1.5068, -1.5912,  0.6566]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gray_weighted = batch_weights.sum(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy of doing the mean calculation as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4142, -0.2200, -0.2374, -1.1039,  0.3578],\n",
       "        [ 0.0676,  0.1268,  0.9222, -1.0375,  0.2899],\n",
       "        [ 1.3845,  0.1224, -0.8231, -0.3893, -0.1400],\n",
       "        [-0.0285,  0.7510,  0.7330, -0.2020, -1.3866],\n",
       "        [-0.1159, -0.0665, -1.3296,  1.7022,  0.0150]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Tensor\n",
    "\n",
    "* It is new feature in Pytorch.\n",
    "* It assigns name to dimension and can make calculation using those names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/c10/core/TensorImpl.h:806: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable.\n"
     ]
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below, we are assigning first dim as channel name, second dim as row and third dim as columns. It assigns name from right to left because the \n",
    "batch dimension is assigned as None.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "print(\"img named:\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gray_named = (img_named[..., :3] * weights_named).sum('channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Renaming the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Storage\n",
    "\n",
    "Tensor Storage makes pytorch quite fast. It assigns block of continous memory for each tensor (matrix or vector) and whenever operation like dimension changes are done, it happens within the same memory without assigning to any other block of memory. checkout book for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_storage = points.storage()\n",
    "points_storage[0] = 2.0\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inplace replacement using _\n",
    "\n",
    "With _, we introduce what is called as inplace replacement of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Storage\n",
    "\n",
    "* Offset - tensor's offset in the underlying storage in terms of number of storage elements (not bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([5., 3.])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1]\n",
    "print(second_point.storage_offset())\n",
    "print(points[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stride is the jump necessary to go from one element to the next one in the\n",
    "specified dimension :attr:`dim`. A tuple of all strides is returned when no\n",
    "argument is passed in. Otherwise, an integer value is returned as the stride in\n",
    "the particular dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(9))\n",
    "a = torch.tensor(a)\n",
    "a.size()\n",
    "a.stride(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.view(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.stride(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how tensor storage works, a and b are referencing the same memory block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(3,3)\n",
    "b = a.view(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a.storage)==id(b.storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manipulating dimension\n",
    "c = b[1:, 1:]\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259, 194, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "image_arr = imageio.imread('/home/mayur/Desktop/opencv_tut/Images/traffic.jpeg')\n",
    "image_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pytorch format for image C * H * W\n",
    "* Convert from Numpy to torch.tensor\n",
    "* Using Permute, aligning the dimension as required by pytorch. it moves dimension as mentioned in permute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 259, 194])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor = torch.from_numpy(image_arr)\n",
    "img_tensor_chw = img_tensor.permute(2, 0, 1)\n",
    "img_tensor_chw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including Batch 1 at different Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 259, 194])\n",
      "torch.Size([3, 1, 259, 194])\n",
      "torch.Size([3, 259, 194, 1])\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor_chw.unsqueeze(0).shape)\n",
    "print(img_tensor_chw.unsqueeze(1).shape)\n",
    "print(img_tensor_chw.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Way to load images\n",
    "\n",
    "Declaring Zero matrix with batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes images also have an alpha channel indicating transparency.\n",
    "\n",
    "* Loading 25 Images from a dataset.\n",
    "* Moving a set of 3 image into batch variable since batch size is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/home/mayur/Desktop/Kaggle Notebooks/Generative Dog Images/all-dogs'\n",
    "filenames = [name for name in os.listdir(data) if os.path.splitext(name)[-1] == '.jpg']\n",
    "pil_img_25 = [Image.open(os.path.join(data, f)) for f in filenames[:25]]\n",
    "pil_transform = transforms.Compose([transforms.Resize((256, 256))])\n",
    "f = lambda: [pil_transform(img) for img in pil_img_25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "for i, file in enumerate(f()):\n",
    "    if i == 3:\n",
    "        break\n",
    "    file = np.asarray(file)\n",
    "    file = torch.from_numpy(file).permute(2, 0, 1)\n",
    "    batch[i] = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 256, 256])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "while applying transforms, we included parameters like image mean and standard deviation. Here, we can calculate, how to find the \n",
    "image mean and standard deviation of image. **In general, we find image mean and standard deviation of the whole dataset, use that as our parameter in transforms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One way to normalize a grayscale image is image/=255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Second way to Normalize\n",
    "batch = batch.float()\n",
    "\n",
    "n_channels = batch.shape[1]\n",
    "for c in range(n_channels):\n",
    "    mean = torch.mean(batch[:, c])\n",
    "    std = torch.std(batch[:, c])\n",
    "    batch[:, c] = (batch[:, c] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding in Torch\n",
    "\n",
    "To handle categorical variables like class names or text feature etc, we use one hot encoding.\n",
    "We create a zero matrix of 25 rows and then plug a value 1 at each index along the dimension mentioned in _scatter. Checkout docstring for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 2, 1, 2, 2, 2, 3, 1, 2, 2, 4, 2, 3, 3, 2, 2, 4, 3, 1, 3, 1, 2, 4, 2,\n",
      "        4])\n"
     ]
    }
   ],
   "source": [
    "target = torch.randint(1, 5, (25,))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot = torch.zeros(target.shape[0], 5)\n",
    "target_onehot.scatter_(1, target.unsqueeze(1), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preforming Normalization on data.\n",
    "* Filtering records based on condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 10])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(25, 10)\n",
    "d_mean = torch.mean(data, dim=0)\n",
    "d_var = torch.var(data, dim=0)\n",
    "data_normalized = (data - d_mean) / torch.sqrt(d_var)\n",
    "print(data_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 10])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_index = target<=2\n",
    "bad_data = data[bad_index]\n",
    "bad_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on Time Series using Bike Sharing dataset\n",
    "\n",
    "* Loading dataset using Numpy\n",
    "* Skip the column names\n",
    "* Convert string to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Years Data\n",
    "bikes_numpy = np.loadtxt(\"/home/mayur/Desktop/Pytorch/data/hour-fixed.csv\",\n",
    "dtype=np.float32,\n",
    "delimiter=\",\",\n",
    "skiprows=1,\n",
    "converters={1: lambda x: float(x[8:10])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert numpy to pytorch tensor.\n",
    "* Check after how many elements the next(second) records starts from zero offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17520, 17)\n",
      "(17, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bikes_numpy.shape)\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "print(bikes.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Convert the hour based into days as 1st dim, 24 hours as 2nd dim and 17 features as 3rd dim. It is quite common to reshape the timeseries data to find seasonality or trends.**\n",
    "* The original data is presented on hour bases.\n",
    "* We have 2 years data, 730 days, each day has 24 hours and each hour represent 17 columns.\n",
    "* We convert the hours into days, which becomes our records or rows. Each row or record is further segregated among hours our second dim.\n",
    "* Each row in second dim contains 17 elements.\n",
    "* Stride value (408, 17, 1), to jump to next day records i need to jump 408 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([730, 24, 17])\n",
      "torch.Size([730, 24, 17]) (408, 17, 1)\n"
     ]
    }
   ],
   "source": [
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "print(daily_bikes.shape)\n",
    "print(daily_bikes.shape, daily_bikes.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Stride : 24 * 17 = 408\n",
    "\n",
    "N: 730 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reshaping the data to make available for training purpose.\n",
    "* Converting Class into One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([730, 17, 24]) (408, 1, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes = daily_bikes.transpose(1, 2)\n",
    "print(daily_bikes.shape, daily_bikes.stride())\n",
    "first_day = bikes[:24].long()\n",
    "weather_onehot = torch.zeros(first_day.shape[0], 4)\n",
    "first_day[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_onehot.scatter_(\n",
    "dim=1,\n",
    "index=first_day[:,9].unsqueeze(1).long() - 1,\n",
    "value=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Combining the one hot encoded matrix with feature matrix.\n",
    "* Creating Zero Matrix for Class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((bikes[:24], weather_onehot), 1)[:1]\n",
    "daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4,\n",
    "daily_bikes.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 4, 24])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_onehot[0]\n",
    "daily_weather_onehot.scatter_(\n",
    "1, daily_bikes[:,9,:].long().unsqueeze(1) - 1, 1.0)\n",
    "daily_weather_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "         0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 21, 24])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)\n",
    "daily_bikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2.,\n",
       "        3., 3., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes[:, 9, :][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_bikes[:, 9, :] = (daily_bikes[:, 9, :] - 1.0) / 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 24])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping Temp values between [0, 1] with standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = daily_bikes[:, 10, :]\n",
    "temp_min = torch.min(temp)\n",
    "temp_max = torch.max(temp)\n",
    "daily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - temp_min)\n",
    "/ (temp_max - temp_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = daily_bikes[:, 10, :]\n",
    "daily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - torch.mean(temp))\n",
    "/ torch.std(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Text using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Level Conversion\n",
    "\n",
    "There are 128 Ascii character, we convert our text/letter into index or integer. Letters not present in ASCII are turned into 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/mayur/Desktop/Pytorch/data/anna.txt\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 128])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.split(\"\\n\")\n",
    "line = lines[100]\n",
    "letter_t = torch.zeros(len(line), 128)\n",
    "letter_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, letter in enumerate(line.lower().strip()):\n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0\n",
    "    #print(letter_index)\n",
    "    letter_t[i][letter_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level Conversion\n",
    "\n",
    "* Clean the text\n",
    "* Sorting the text\n",
    "* Mapping the words into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(input_str):\n",
    "    punctuation = '.,;:\"!?”“_-'\n",
    "    word_list = input_str.lower().replace('\\n',' ').split()\n",
    "    word_list = [word.strip(punctuation) for word in word_list]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('despair, and found no answer.', ['despair', 'and', 'found', 'no', 'answer'])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_line = clean_words(line)\n",
    "line, words_in_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15070"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = sorted(set(clean_words(text)))\n",
    "word2index_dict = {word: i for (i, word) in enumerate(word_list)}\n",
    "len(word2index_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating a Matrix where columns size is length of all words and each row is length of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15070])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_t = torch.zeros(len(words_in_line), len(word2index_dict))\n",
    "word_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 3588 despair\n",
      " 1  680 and\n",
      " 2 5362 found\n",
      " 3 8926 no\n",
      " 4  732 answer\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words_in_line):\n",
    "    word_index = word2index_dict[word]\n",
    "    word_t[i][word_index] = 1\n",
    "    print('{:2} {:4} {}'.format(i, word_index, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a Model\n",
    "\n",
    "* Creating linear regression\n",
    "* Mapping of x and y.\n",
    "* Turing List into Tensor.\n",
    "* Create a function for basic line equation\n",
    "* Create a function to measure the loss using mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w*t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(t_p, t_c):\n",
    "    return torch.mean((t_p-t_c)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8846)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(w, t_u, b)\n",
    "Loss = loss(t_p, t_c)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning rate - delta\n",
    "* Update weights w with delta.\n",
    "* Measure Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "loss_rate_of_change_w = \\\n",
    "(loss(model(t_u, w + delta, b), t_c) -\n",
    "loss(model(t_u, w - delta, b), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4517.2979)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(loss(model(t_u, w + delta, b), t_c) - loss(model(t_u, w - delta, b), t_c)) / (2 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - learning_rate * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = \\\n",
    "(loss(model(t_u, w, b + delta), t_c) -\n",
    "loss(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "b = b - learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Derivative\n",
    "\n",
    "d loss / d w = (d loss / d t_p) * (d t_p / d w)\n",
    "\n",
    "**grad_fn** - Updating the weight and bias using learning rate after calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss(t_p, t_c):\n",
    "    dsq_diff = 2 * (t_p - t_c) / t_p.size(0)\n",
    "    \n",
    "    return dsq_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### if d(w*t_u + b)/ dw = t_u + 0 = t_u\n",
    "\n",
    "def d_model_dw(w, t_u, b):\n",
    "    return t_u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### if d(w*t_u + b)/ db = 0 + 1.0 = 1.0\n",
    "\n",
    "def d_model_db(w, t_u, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    \n",
    "    dloss_dtp = d_loss(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * d_model_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * d_model_db(t_u, w, b)\n",
    "    \n",
    "\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating training loop, to iterate over the training data to learn weight by updating it based on the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, w, b) #Forward pass\n",
    "        \n",
    "        Loss = loss(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b) #Backward pass\n",
    "        \n",
    "        params = params - learning_rate * grad\n",
    "        if epoch % 10 == 1:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(Loss)))\n",
    "            print(f'params: {params}')\n",
    "            print(f'grad: {grad}')\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884644\n",
      "params: tensor([ 0.1740, -0.8260])\n",
      "grad: tensor([82.6000, 82.6000])\n",
      "Epoch 11, Loss 30.432922\n",
      "params: tensor([ 0.2113, -0.6289])\n",
      "grad: tensor([-0.1006, -0.4782])\n",
      "Epoch 21, Loss 29.714087\n",
      "params: tensor([ 0.2143, -0.6147])\n",
      "grad: tensor([-0.0065, -0.0305])\n",
      "Epoch 31, Loss 29.671791\n",
      "params: tensor([ 0.2145, -0.6138])\n",
      "grad: tensor([-0.0004, -0.0019])\n",
      "Epoch 41, Loss 29.669165\n",
      "params: tensor([ 0.2146, -0.6137])\n",
      "grad: tensor([-2.5302e-05, -1.1790e-04])\n",
      "Epoch 51, Loss 29.669001\n",
      "params: tensor([ 0.2146, -0.6137])\n",
      "grad: tensor([-1.8179e-06, -8.5831e-06])\n",
      "Epoch 61, Loss 29.668993\n",
      "params: tensor([ 0.2146, -0.6137])\n",
      "grad: tensor([-2.9802e-07, -1.6689e-06])\n",
      "Epoch 71, Loss 29.668993\n",
      "params: tensor([ 0.2146, -0.6137])\n",
      "grad: tensor([-2.9802e-07, -1.6689e-06])\n",
      "Epoch 81, Loss 29.668993\n",
      "params: tensor([ 0.2146, -0.6137])\n",
      "grad: tensor([-2.9802e-07, -1.6689e-06])\n",
      "Epoch 91, Loss 29.668993\n",
      "params: tensor([ 0.2146, -0.6137])\n",
      "grad: tensor([-2.9802e-07, -1.6689e-06])\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(\n",
    "n_epochs = 100,\n",
    "learning_rate = 1e-2,\n",
    "params = torch.tensor([1.0, 0.0]),\n",
    "t_u = t_u,\n",
    "t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd for derivation !\n",
    "\n",
    "Pytorch provides a **.grad** characteristic to each tensor. If a tensor is created as **require_grad = True**, then that tensor turns into learnable parameter. We can check if the parameters are getting updated after executing loss.backward(). Loss.backward() tells pytorch to update the all learnable parameters to update weight based on loss. We can turn **.grad** to zero by **zero_()** because if we don't turn the **.grad to zero then grad values gets accumulated into .grad after each epoch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0],requires_grad=True)\n",
    "params.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss = loss(model(t_u, *params), t_c)\n",
    "Loss.backward()\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_AG(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "            \n",
    "        t_p = model(t_u, *params)\n",
    "        Loss = loss(t_p, t_c)\n",
    "        Loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'epoch {epoch}, Loss: {Loss}')\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un = 0.1 * t_u #scaling down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, Loss: 22.148710250854492\n",
      "epoch 200, Loss: 16.608064651489258\n",
      "epoch 300, Loss: 12.664560317993164\n",
      "epoch 400, Loss: 9.857802391052246\n",
      "epoch 500, Loss: 7.8601155281066895\n",
      "epoch 600, Loss: 6.438284397125244\n",
      "epoch 700, Loss: 5.426309585571289\n",
      "epoch 800, Loss: 4.706046104431152\n",
      "epoch 900, Loss: 4.1934051513671875\n",
      "epoch 1000, Loss: 3.828537940979004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  4.8021, -14.1031], requires_grad=True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop_AG(\n",
    "n_epochs = 1000,\n",
    "learning_rate = 1e-2,\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "t_u = t_un,\n",
    "t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Optimizer is the tool used to update the weights. Pytorch provides wide range of optimizer from SGD to ADAM & many more. Check docstring for more details.\n",
    "* Iterate through EPOCHs.\n",
    "* Training the model and find predicted value.\n",
    "* Pass the predicted value to loss function to calculate the loss.\n",
    "* Turn the existing parameter/weights to zero.\n",
    "* calculate the weights using optimizer based on loss.\n",
    "* Apply the weights update to parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_optim(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        t_p = model(t_u, *params)\n",
    "        Loss = loss(t_p, t_c)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {Loss}')\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 22.148710250854492\n",
      "Epoch: 200, Loss: 16.608068466186523\n",
      "Epoch: 300, Loss: 12.664565086364746\n",
      "Epoch: 400, Loss: 9.857809066772461\n",
      "Epoch: 500, Loss: 7.8601179122924805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4.0443, -9.8133], requires_grad=True)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop_optim(n_epochs=500, optimizer=optimizer,params= params,  t_u = t_un,t_c= t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataset into Train and Validation\n",
    "\n",
    "* we are using indexing to split the dataset.\n",
    "* Not recommend approach for splitting dataset into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  6, 10,  2,  3,  8,  5,  9,  1]), tensor([4, 7]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss(train_t_p, train_t_c)\n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss = loss(val_t_p, val_t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch <= 3 or epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Validation loss {val_loss.item():.4f}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 90.7754, Validation loss 33.5146\n",
      "Epoch 2, Training loss 33.7982, Validation loss 34.4394\n",
      "Epoch 3, Training loss 27.0283, Validation loss 42.0913\n",
      "Epoch 100, Training loss 21.0966, Validation loss 35.6010\n",
      "Epoch 200, Training loss 17.0360, Validation loss 26.2173\n",
      "Epoch 300, Training loss 13.8714, Validation loss 19.2201\n",
      "Epoch 400, Training loss 11.4051, Validation loss 14.0459\n",
      "Epoch 500, Training loss 9.4831, Validation loss 10.2596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.9074, -8.6521], requires_grad=True)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "n_epochs = 500,\n",
    "optimizer = optimizer,\n",
    "params = params,\n",
    "train_t_u = train_t_un,\n",
    "val_t_u = val_t_un,\n",
    "train_t_c = train_t_c,\n",
    "val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Grad Enabled - Acts like switch to ON and OFF the autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_forward(t_u, t_c, is_train):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - w2 * t_u ** 2 + w1 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.randn(3,) \n",
    "params.requires_grad=True\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam([params], lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(t_u, w1, w2, b):\n",
    "    return w2 * t_u ** 2 + w1 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "duplicate argument 'optimizer' in function definition (<ipython-input-230-73a45e2719d4>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-230-73a45e2719d4>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    train_t_p = new_model(train_t_u, w1, w2, b)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m duplicate argument 'optimizer' in function definition\n"
     ]
    }
   ],
   "source": [
    "def train(n_epochs, optimizer, optimizer, train_t_u, val_t_u,train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w1, w2, b = params\n",
    "        train_t_p = new_model(train_t_u, w1, w2, b)\n",
    "        train_loss = criterion(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = new_model(val_t_u, *params)\n",
    "            val_loss = criterion(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100==0:\n",
    "            print(f'Epoch: {epoch}, Loss: {train_loss}')\n",
    "            print(params)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 4.886602401733398\n",
      "tensor([ 0.4475,  0.3748, -1.6703], requires_grad=True)\n",
      "Epoch: 200, Loss: 3.933260679244995\n",
      "tensor([ 0.2843,  0.4087, -2.0471], requires_grad=True)\n",
      "Epoch: 300, Loss: 3.2933266162872314\n",
      "tensor([ 0.1325,  0.4411, -2.3996], requires_grad=True)\n",
      "Epoch: 400, Loss: 2.9597949981689453\n",
      "tensor([ 0.0151,  0.4665, -2.6855], requires_grad=True)\n",
      "Epoch: 500, Loss: 2.8161251544952393\n",
      "tensor([-0.0629,  0.4840, -2.8981], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0629,  0.4840, -2.8981], requires_grad=True)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(n_epochs = 500,\n",
    "optimizer = optimizer,\n",
    "params=params,\n",
    "train_t_u = train_t_un,\n",
    "val_t_u = val_t_un,\n",
    "train_t_c = train_t_c,\n",
    "val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing Neural Nets Using nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model with one sample with one feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.3783]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.7125], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "l_model = nn.Linear(1,1)\n",
    "l_model(x)\n",
    "print(l_model.weight)\n",
    "print(l_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_add_d = x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_add_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model which takes batch of samples with one feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0908],\n",
       "        [1.0908],\n",
       "        [1.0908],\n",
       "        [1.0908],\n",
       "        [1.0908],\n",
       "        [1.0908],\n",
       "        [1.0908],\n",
       "        [1.0908],\n",
       "        [1.0908],\n",
       "        [1.0908]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Network with One Hidden Layer\n",
    "* Converting list to tensor\n",
    "* Pytorch has Sequence Module similar to Keras, We can add layers in sequence, in the order of operation.\n",
    "* Finding Total number of parameters in the model.\n",
    "* Assigning names to layers, it is helpful when you want to retrain particular layer.\n",
    "* Training the network with optimizer and criterion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(t_u).unsqueeze(1)\n",
    "y = torch.tensor(t_c).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(1, 10),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(10, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([10, 1]), torch.Size([10]), torch.Size([1, 10]), torch.Size([1])]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.shape for p in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([10, 1])\n",
      "0.bias torch.Size([10])\n",
      "2.weight torch.Size([1, 10])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "seq_model = nn.Sequential(OrderedDict([\n",
    "('hidden_linear', nn.Linear(1, 9)),\n",
    "('hidden_activation', nn.Tanh()),\n",
    "('output_linear', nn.Linear(9, 1))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (hidden_linear): Linear(in_features=1, out_features=9, bias=True)\n",
       "  (hidden_activation): Tanh()\n",
       "  (output_linear): Linear(in_features=9, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight torch.Size([9, 1])\n",
      "hidden_linear.bias torch.Size([9])\n",
      "output_linear.weight torch.Size([1, 9])\n",
      "output_linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3329], requires_grad=True)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,\n",
    "t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        t_p_val = model(t_u_val)\n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
    "            f\" Validation loss {loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 1.8214, Validation loss 6.1820\n",
      "Epoch 1000, Training loss 1.8137, Validation loss 6.3981\n",
      "Epoch 2000, Training loss 1.8061, Validation loss 6.6104\n",
      "Epoch 3000, Training loss 1.7986, Validation loss 6.8202\n",
      "Epoch 4000, Training loss 1.7910, Validation loss 7.0274\n",
      "Epoch 5000, Training loss 1.7834, Validation loss 7.2324\n",
      "output tensor([[12.5741],\n",
      "        [-0.5377]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[11.],\n",
      "        [-4.]])\n",
      "hidden tensor([[ 2.8302e-03],\n",
      "        [ 8.7560e-03],\n",
      "        [-1.5450e-02],\n",
      "        [ 8.4116e-03],\n",
      "        [-9.8095e-03],\n",
      "        [-2.1610e-03],\n",
      "        [ 3.1948e-05],\n",
      "        [ 1.0729e-04],\n",
      "        [ 1.5020e-03]])\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "n_epochs = 5000,\n",
    "optimizer = optimizer,\n",
    "model = seq_model,\n",
    "loss_fn = nn.MSELoss(),\n",
    "t_u_train = train_t_un.unsqueeze(1),\n",
    "t_u_val = val_t_un.unsqueeze(1),\n",
    "t_c_train = train_t_c.unsqueeze(1),\n",
    "t_c_val = val_t_c.unsqueeze(1))\n",
    "\n",
    "print('output', seq_model(val_t_un.unsqueeze(1)))\n",
    "print('answer', val_t_c.unsqueeze(1))\n",
    "print('hidden', seq_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
